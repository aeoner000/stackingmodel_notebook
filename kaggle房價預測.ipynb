{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aeoner000/stackingmodel_notebook/blob/main/kaggle%E6%88%BF%E5%83%B9%E9%A0%90%E6%B8%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKy4OQclPY7z"
      },
      "source": [
        "# 1.匯入套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcTAXcpnOPYw"
      },
      "outputs": [],
      "source": [
        "# 基本套件\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import traceback\n",
        "import datetime\n",
        "# !ls -R /content/drive/MyDrive/機器學習練習檔/kaggle房價預測/project\n",
        "# 數值運算與資料處理\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import boxcox\n",
        "from scipy.special import inv_boxcox\n",
        "from sklearn.impute import KNNImputer\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "# 資料視覺化\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shap\n",
        "\n",
        "# 類別編碼與特徵處理\n",
        "import category_encoders as ce\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 監督式模型\n",
        "from sklearn.linear_model import (LinearRegression as lr,Ridge as rdg,RidgeCV as rdgcv,Lasso,SGDRegressor as sgd,LogisticRegression as logis)\n",
        "from sklearn.ensemble import RandomForestRegressor as rf, StackingRegressor as skStack\n",
        "from sklearn.tree import DecisionTreeRegressor as dt\n",
        "from sklearn.svm import SVR as svr\n",
        "from sklearn.neighbors import KNeighborsRegressor as knn\n",
        "from sklearn.neural_network import MLPRegressor as mlp\n",
        "from sklearn.kernel_ridge import KernelRidge as kr\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "\n",
        "# 模型評估與交叉驗證\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error\n",
        "\n",
        "# 並行處理與儲存\n",
        "from joblib import Parallel, delayed, dump, load\n",
        "\n",
        "# 超參數優化\n",
        "import optuna\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "from skopt.utils import use_named_args\n",
        "\n",
        "# pip install xgboost lightgbm category_encoders scikit-optimize catboost optuna\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.資料探勘"
      ],
      "metadata": {
        "id": "DHfBLE78-3KG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/機器學習練習檔/kaggle房價預測/train.csv\")\n",
        "\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/機器學習練習檔/kaggle房價預測/train.csv\", keep_default_na=False, na_values=[])\n",
        "# filepath：CSV 檔案路徑\n",
        "# keep_default_na：是否保留 pandas 預設的 NA 標記\n",
        "# na_values：指定哪些值要被視為 NA\n",
        "\n",
        "#### 直方圖、散佈圖、盒鬚圖 --->多特徵子圖\n",
        "# =====================================================================================================================================================\n",
        "def plot_features(data, plot_type='hist', target=None, drop_cols=None, n_col=4, bins=20, kde=True):\n",
        "    \"\"\"\n",
        "    data: pd.DataFrame\n",
        "    plot_type: 'hist' -> 直方圖, 'scatter' -> 散佈圖(y軸:目標變數), 'box' -> 盒鬚圖\n",
        "    target: 目標變數名稱，plot_type='scatter' 時必填\n",
        "    drop_cols: scatter 時要排除的數值欄位 (list)\n",
        "    n_col: 每列的圖數\n",
        "    bins: hist 圖的 bins 數\n",
        "    kde: hist 圖是否畫 KDE\n",
        "    \"\"\"\n",
        "\n",
        "    if plot_type not in ['hist', 'scatter', 'box']:\n",
        "        raise ValueError(\"plot_type 只能是 'hist', 'scatter', 'box'\")\n",
        "\n",
        "    if plot_type == 'scatter' and target is None:\n",
        "        raise ValueError(\"plot_type='scatter' 時必須提供 target\")\n",
        "\n",
        "    if plot_type == 'scatter' or plot_type == 'box':\n",
        "        numeric_cols = data.select_dtypes(include=np.number).columns\n",
        "        if drop_cols:\n",
        "            numeric_cols = numeric_cols.drop(drop_cols)\n",
        "        cols_to_plot = numeric_cols\n",
        "    else:\n",
        "        cols_to_plot = data.columns\n",
        "\n",
        "    n_var = len(cols_to_plot) # 變數數量\n",
        "    n_row = n_var // n_col + (1 if n_var % n_col != 0 else 0 )# 子圖要幾行\n",
        "    fig, ax = plt.subplots(n_row, n_col, figsize=(n_col*6, n_row*4)) # fig=大圖框，ax=多個小圖框==>2Darray\n",
        "    axs = ax.flatten() # 攤平成一列，for迴圈比較好做\n",
        "\n",
        "    for i, col in enumerate(cols_to_plot): # enumerate 會把每個元素包裝成一個 tuple 包含: 1.索引（index）2.元素本身（value）\n",
        "        if plot_type == 'hist':\n",
        "            sns.histplot(data[col], ax=axs[i], bins=bins, kde=kde) # axs[i]就是第i個子圖，ax參數若沒填就是自動生成新圖\n",
        "        elif plot_type == 'scatter':\n",
        "            sns.scatterplot(data=data, x=col, y=target, ax=axs[i])\n",
        "        elif plot_type == 'box':\n",
        "            sns.boxplot(data=data[col], ax=axs[i])\n",
        "        axs[i].set_title(col)\n",
        "\n",
        "    for j in range(i+1, len(axs)):\n",
        "        fig.delaxes(axs[j]) # 刪除上面迴圈最後i的第i+1個到最後一張空白子圖\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if plot_type in ['scatter', 'box']:\n",
        "        print(f'數值變數數量: {n_var}')\n",
        "        print(f'數值變數: {list(cols_to_plot)}')\n",
        "\n",
        "# # 直方圖\n",
        "# plot_features(train_df, plot_type='hist', bins=30)\n",
        "\n",
        "# # 散佈圖 y軸:SalePrice\n",
        "# plot_features(train_df, plot_type='scatter', target='SalePrice', drop_cols=['Id', 'SalePrice'])\n",
        "\n",
        "# # 盒鬚圖\n",
        "# plot_features(train_df, plot_type='box', drop_cols=['Id', 'SalePrice'])\n",
        "\n",
        "#### 熱力圖\n",
        "# =====================================================================================================================================================\n",
        "def plot_heatmap(data, numeric_only=True, target=None, annot=True, cmap='coolwarm', figsize=(12,10)):\n",
        "    \"\"\"\n",
        "    繪製數值特徵的相關性熱力圖\n",
        "    data: DataFrame\n",
        "    numeric_only: 是否只選數值欄位\n",
        "    target: 如果指定目標欄位，會畫該欄與其他數值欄的相關性\n",
        "    annot: 是否在格子上標示數值\n",
        "    cmap: 顏色地圖\n",
        "    figsize: 圖片大小\n",
        "    \"\"\"\n",
        "    if numeric_only:\n",
        "        df = data.select_dtypes(include=np.number)\n",
        "    else:\n",
        "        df = data.copy()\n",
        "\n",
        "    if target and target in df.columns:\n",
        "        corr = df.corr()[[target]].sort_values(by=target, ascending=False)\n",
        "    else:\n",
        "        corr = df.corr()\n",
        "\n",
        "    n = corr.shape[0]\n",
        "    size = max(6, 40//n)  # 特徵太多文字縮小\n",
        "    plt.figure(figsize=figsize)  # figsize 設定圖框長寬\n",
        "    sns.heatmap(\n",
        "      corr,               # 要畫的數據 (通常是相關矩陣)\n",
        "      annot=annot,           # 是否在格子中標註數值\n",
        "      fmt=\".2f\",            # 標註的數字格式 (小數點 2 位)\n",
        "      cmap=cmap,            # 顏色映射 (colormap)，例如 \"coolwarm\", \"viridis\"\n",
        "      cbar=True,            # 是否顯示右邊的顏色條 (colorbar)\n",
        "      square=True,           # 是否讓每個格子是正方形\n",
        "      annot_kws={\"size\": size}    # 控制標註字體大小\n",
        "    )\n",
        "    plt.title('Heatmap of Correlations', fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "# 針對所有數值欄位相關性\n",
        "# plot_heatmap(train_df, numeric_only=True)\n",
        "\n",
        "# 只看數值欄位與目標 SalePrice 的相關性\n",
        "# plot_heatmap(train_df, target='SalePrice')\n",
        "\n",
        "#### VIF值計算\n",
        "# =====================================================================================================================================================\n",
        "def vif_cal(data):\n",
        "  df = data.select_dtypes(include=np.number)\n",
        "  df = df.dropna()\n",
        "  # 重要：計算 VIF 前，必須加上常數項（截距）\n",
        "  X = add_constant(df)\n",
        "\n",
        "  # 創建一個 DataFrame 來儲存 VIF 值\n",
        "  vif_df = pd.DataFrame()\n",
        "  vif_df[\"feature\"] = X.columns\n",
        "\n",
        "  # 計算 VIF 值並添加到 DataFrame           # i 是第i個特徵\n",
        "  vif_df[\"VIF\"] = [f'{variance_inflation_factor(X.values, i):.2f}' for i in range(X.shape[1])] # X.shape[1]是col\n",
        "                        # X.values==>把 DataFrame X 轉成 NumPy 陣列（2D）\n",
        "  return vif_df\n",
        "\n",
        "#### SHAP特徵重要性\n",
        "# =====================================================================================================================================================\n",
        "def shap_analysis(data, y_name, model=None, Dependence=None):\n",
        "  data = data.select_dtypes(include=np.number).dropna()\n",
        "  y = data[y_name]\n",
        "  X = data.drop(columns=[y_name])\n",
        "\n",
        "  columns_to_drop = [col for col in ['Id', 'id', 'ID'] if col in X.columns]\n",
        "  if columns_to_drop:\n",
        "    X = X.drop(columns=columns_to_drop)\n",
        "  if Dependence in X.columns:\n",
        "    Dependence = Dependence\n",
        "  elif Dependence is None:\n",
        "    Dependence = X.columns[0]\n",
        "  else:\n",
        "    raise ValueError(\"Dependence not in X.columns\")\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  if model is None:\n",
        "    model = xgb()\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # 計算 SHAP 值\n",
        "  # 1. 建立 explainer 物件\n",
        "  # 這裡使用 TreeExplainer，因為我們的模型是樹狀模型\n",
        "  explainer = shap.TreeExplainer(model)\n",
        "\n",
        "  # 2. 計算測試集上每個預測的 SHAP 值\n",
        "  # shap_values 是一個包含 SHAP 值的 NumPy 陣列\n",
        "  shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "  # SHAP 值視覺化\n",
        "  # 3. 繪製摘要圖 (Summary Plot)\n",
        "  # 這個圖可以顯示每個特徵對模型輸出的整體影響\n",
        "  print(\"--- SHAP Summary Plot ---\")\n",
        "  shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "  # 4. 繪製依賴圖 (Dependence Plot)\n",
        "  # 這個圖可以顯示單一特徵如何影響預測，並顯示特徵之間的交互作用\n",
        "  print(\"\\n--- SHAP Dependence Plot for bmi ---\")\n",
        "  shap.dependence_plot(Dependence, shap_values, X_test)\n",
        "\n",
        "  # 5. 繪製單一預測的解釋 (Force Plot)\n",
        "  # 這裡我們解釋測試集中的第一筆資料 (索引 0)\n",
        "  print(\"\\n--- SHAP Force Plot for a single prediction ---\")\n",
        "  shap.initjs()\n",
        "  display(shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:]))\n",
        "\n",
        "# shap_analysis(train_df, y_name='SalePrice', Dependence='LotFrontage')\n"
      ],
      "metadata": {
        "id": "Sk_aRcX5-3Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08qjV9Lk5TXN"
      },
      "source": [
        "# 3.ETL流程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2E1Y5vh5Sw3"
      },
      "outputs": [],
      "source": [
        "class ETLPipeline:\n",
        "    def __init__(self):\n",
        "        self.save_dir = save_dir\n",
        "        self.scaler = None\n",
        "        self.encoder = None\n",
        "        self.pca = None\n",
        "        self.imputer = None\n",
        "        self.y_name = None\n",
        "        self.X_interaction_list = None\n",
        "        self.pca_n_components = 20\n",
        "\n",
        "        # NA為類別而不是缺失值的特徵列表\n",
        "        self.str_na_col = [\n",
        "            'Alley', 'BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2', 'GarageType',\n",
        "            'GarageFinish','GarageQual', 'GarageCond', 'Fence', 'MiscFeature', 'PoolQC',\n",
        "            'BsmtExposure', 'FireplaceQu', 'MasVnrType'\n",
        "        ]\n",
        "        self.na_per15up = None\n",
        "        self.idx = None\n",
        "\n",
        "        # 連續變數\n",
        "        self.con_var = [\n",
        "          'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
        "          'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageCars',\n",
        "          'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch',\n",
        "          'PoolArea', 'MiscVal', 'TotRmsAbvGrd', 'Fireplaces', 'KitchenAbvGr', 'FullBath',\n",
        "          'BsmtFullBath', 'BsmtHalfBath', 'BedroomAbvGr', 'HalfBath'\n",
        "        ]\n",
        "        # 有序類別變數\n",
        "        self.ord_var = [\n",
        "          'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'HeatingQC', 'KitchenQual',\n",
        "          'FireplaceQu', 'PoolQC', 'Fence', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
        "          'BsmtFinType2', 'Functional', 'LandSlope', 'GarageFinish', 'GarageQual', 'GarageCond'\n",
        "        ]\n",
        "        # 無序類別變數\n",
        "        self.nom_var = [\n",
        "          'MSSubClass', 'MSZoning', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n",
        "          'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n",
        "          'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n",
        "          'Heating', 'Electrical', 'GarageType', 'PavedDrive', 'SaleType', 'SaleCondition',\n",
        "          'MiscFeature'\n",
        "        ]\n",
        "        # 直接轉為1、0\n",
        "        self.one_zero = ['CentralAir', 'Street', 'HasGarage']\n",
        "        # 時間\n",
        "        self.time_ = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', 'MoSold']\n",
        "\n",
        "        self.id_1 = ['Id']\n",
        "        self.y_var = ['SalePrice']\n",
        "\n",
        "        #有序編碼\n",
        "        self.ordinal_mappings = {\n",
        "            'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1},\n",
        "            'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1},\n",
        "            'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1},\n",
        "            'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1},\n",
        "            'FireplaceQu': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NO': 0},\n",
        "            'PoolQC': {'Ex': 4, 'Gd': 3, 'Ta': 2, 'Fa': 1, 'NO': 0},\n",
        "            'Fence': {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'NO': 0},\n",
        "            'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NO': 0},\n",
        "            'BsmtCond': {'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NO': 0},\n",
        "            'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NO': 0},\n",
        "            'BsmtFinType1': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NO': 0},\n",
        "            'BsmtFinType2': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NO': 0},\n",
        "            'Functional': {'Typ': 7, 'Min1': 6, 'Min2': 5, 'Mod': 4, 'Maj1': 3, 'Maj2': 2, 'Sev': 1, 'Sal': 0},\n",
        "            'LandSlope': {'Gtl': 3, 'Mod': 2, 'Sev': 1},\n",
        "            'GarageFinish': {'Fin': 3, 'RFn': 2, 'Unf': 1, 'NO': 0},\n",
        "            'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NO': 0},\n",
        "            'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NO': 0}\n",
        "        }\n",
        "    def _preprocess(self, df: pd.DataFrame):\n",
        "\n",
        "        # 1. 挑出NA為類別而不是缺失值的欄，並替換NA為NO\n",
        "        df[self.str_na_col] = df[self.str_na_col].fillna('NO')\n",
        "\n",
        "        # 2. 新增一欄特徵為有無車庫\n",
        "        df['HasGarage'] = df['GarageYrBlt'].notna().astype(int)\n",
        "\n",
        "        # 2.1 將無車庫NA的部分設為-1，避免被認為是缺失值\n",
        "        df['GarageYrBlt'] = df['GarageYrBlt'].fillna(-1)\n",
        "\n",
        "        # 3. 丟棄NA值數量大於總樣本15%的特徵\n",
        "        count_na_2 = df.isna().sum()\n",
        "        total_sample = len(df)\n",
        "        self.na_per15up = count_na_2[count_na_2 > total_sample * 0.15].index\n",
        "        df = df.drop(columns=self.na_per15up)\n",
        "\n",
        "        # 4. 刪除y中有缺失值的一整列\n",
        "        if 'SalePrice' in df.columns:\n",
        "          df = df.dropna(subset=['SalePrice'])\n",
        "\n",
        "        # 5. 映射有序類別變數為數值\n",
        "        for col, mapping in self.ordinal_mappings.items():\n",
        "          df[col] = df[col].map(mapping)\n",
        "\n",
        "        # 6. 將二元類別依順序轉為1、0\n",
        "        for col in self.one_zero:\n",
        "          if col == 'CentralAir':\n",
        "            df[col] = df[col].map({'Y': 1, 'N': 0})\n",
        "          else:\n",
        "            df[col] = pd.factorize(df[col])[0] # 將二元類別依順序轉為1、0\n",
        "\n",
        "        # 7. 將落成或改建日期與買賣日期整合為屋齡\n",
        "        df['houseage'] = df['YrSold'] - df['YearBuilt']\n",
        "        df['remodelage'] = df['YrSold'] - df['YearRemodAdd']\n",
        "\n",
        "        # 8. 剔除日期特徵\n",
        "        df = df.drop(columns=self.time_)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def serch_interaction_feature(self, X, y=None, top_n=5):\n",
        "\n",
        "        base_features = X.columns.tolist()\n",
        "        # print('原始特徵數: ', len(base_features)+1)\n",
        "\n",
        "        # 分割數據\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # 建立 LightGBM 模型\n",
        "        train_data = lgb.Dataset(X_train, label=y_train)\n",
        "        test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "        params = {\"objective\": \"regression\", \"metric\": \"rmse\", \"boosting\": \"gbdt\", \"num_leaves\": 31, \"learning_rate\": 0.05, \"feature_fraction\": 0.9, \"verbose\": -1}\n",
        "        model = lgb.train(params,train_data, valid_sets=[test_data], num_boost_round=100, callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)])\n",
        "\n",
        "        # 建立 SHAP explainer\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "        shap_interaction = explainer.shap_interaction_values(X_test)\n",
        "        if isinstance(shap_interaction, list):\n",
        "            shap_interaction = shap_interaction[0]  # 取單目標 array\n",
        "        # 計算交互作用強度\n",
        "        interaction_strength = np.abs(shap_interaction).mean(axis=0)\n",
        "\n",
        "        n_features = X_test.shape[1]\n",
        "\n",
        "        # 生成所有可能的特徵對並計算交互強度\n",
        "        pairs = []\n",
        "        cols = X_test.columns.tolist()\n",
        "\n",
        "        for i in range(n_features):\n",
        "            for j in range(i+1, n_features):  # 只取上三角，避免重複\n",
        "                feature1, feature2 = cols[i], cols[j]\n",
        "                # 確保至少有一個特徵是連續變數\n",
        "                if feature1 in self.con_var or feature2 in self.con_var:\n",
        "                    # 確保兩個特徵都在原始特徵中\n",
        "                    if feature1 in base_features and feature2 in base_features:\n",
        "                        pairs.append(((feature1, feature2), interaction_strength[i, j]))\n",
        "\n",
        "        # 按交互強度排序，取前n個\n",
        "        top_interactions = sorted(pairs, key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "        # 生成交互項特徵\n",
        "        print(\"生成的二階交互項特徵：\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        X_interaction_list = []\n",
        "        for idx, ((f1, f2), score) in enumerate(top_interactions, 1):\n",
        "            X_interaction_list.append((f1, f2))\n",
        "            print(f\"{idx:2d}. {f1} × {f2} → (強度: {score:.4f})\")\n",
        "\n",
        "        # print(f\"\\n總共生成了 {len(top_interactions)} 個交互項特徵\")\n",
        "        # print('增加交互向後特徵數量: ', len(X.columns)+top_n)\n",
        "        return X_interaction_list\n",
        "\n",
        "    def add_interaction_feature(self, X, X_interaction_list):\n",
        "        interaction_df = pd.DataFrame()\n",
        "        for f1, f2 in X_interaction_list:\n",
        "            interaction_df[f\"interx{f1}_{f2}\"] = X[f1] * X[f2]\n",
        "        return interaction_df\n",
        "\n",
        "    def fit(self, df: pd.DataFrame, y_name=\"SalePrice\"):\n",
        "        df = df.copy()\n",
        "        df.set_index('Id', inplace=True)\n",
        "        self.y_name = y_name\n",
        "        # 1. 執行清理\n",
        "        df = self._preprocess(df)\n",
        "\n",
        "        # 2. 分開 y\n",
        "        self.y = df[y_name]\n",
        "        self.X = df.drop(columns=[y_name])\n",
        "\n",
        "        # 3. 無序類別變數轉換(LeaveOneOutEncoder(LOO)編碼器)\n",
        "        self.encoder = ce.LeaveOneOutEncoder(cols=self.nom_var, sigma=0.01)\n",
        "        self.encoder.fit(self.X[self.nom_var], self.y)\n",
        "        X_cat = self.encoder.transform(self.X[self.nom_var])\n",
        "\n",
        "        # 3.1 將原始值替換\n",
        "        X_temp = self.X.copy()\n",
        "        X_temp[self.nom_var] = X_cat\n",
        "\n",
        "        # 4. 數值標準化\n",
        "        self.scaler = StandardScaler()\n",
        "        self.scaler.fit(X_temp)\n",
        "        X_scaled = self.scaler.transform(X_temp)\n",
        "\n",
        "        # 5. 填補缺失值(knn插值)\n",
        "        self.imputer = KNNImputer(n_neighbors=10, weights='distance')\n",
        "        self.imputer.fit(X_scaled)\n",
        "        X_imputed = self.imputer.transform(X_scaled)\n",
        "        X_invers = self.scaler.inverse_transform(X_imputed)\n",
        "        X_invers_imputed_df = pd.DataFrame(X_invers, columns=self.X.columns, index=self.X.index)\n",
        "\n",
        "        # 6. 計算二階交互項前5重要特徵\n",
        "        self.X_interaction_list = self.serch_interaction_feature(X_invers_imputed_df, self.y, top_n=5)\n",
        "\n",
        "        # 7. 加入原始資料PCA後特徵，加入20項\n",
        "        self.pca = PCA(n_components=self.pca_n_components)\n",
        "        self.pca.fit(X_imputed)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, df: pd.DataFrame):\n",
        "        df = df.copy()\n",
        "        if 'Id' in df.columns:\n",
        "          df.set_index('Id', inplace=True)\n",
        "        print('原始特徵數: ', len(df.columns))\n",
        "        df = self._preprocess(df)\n",
        "        print('因缺失超過15%刪除特徵為: ', self.na_per15up)\n",
        "\n",
        "        X = df.drop(columns=[self.y_name], errors='ignore')\n",
        "\n",
        "        # 1. 類別編碼\n",
        "        X[self.nom_var] = self.encoder.transform(X[self.nom_var])\n",
        "\n",
        "        # 2. 標準化\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "\n",
        "        # 3. 填補缺失值\n",
        "        X_imputed = self.imputer.transform(X_scaled)\n",
        "        X_imputed_inver = self.scaler.inverse_transform(X_imputed)\n",
        "\n",
        "        X_imputed_inver_df = pd.DataFrame(X_imputed_inver, columns=X.columns, index=X.index)\n",
        "        class_var = self.ord_var + self.one_zero\n",
        "        X_imputed_inver_df[class_var] = X_imputed_inver_df[class_var].round(0).astype(int)\n",
        "\n",
        "        # 4. 加入交互特徵\n",
        "        X_interaction_df = self.add_interaction_feature(X_imputed_inver_df, self.X_interaction_list)\n",
        "        print(f'增加交互項特徵數量: ', len(self.X_interaction_list))\n",
        "\n",
        "        # 5. 加入PCA特徵\n",
        "        X_pca = self.pca.transform(X_imputed)\n",
        "        pca_df = pd.DataFrame(X_pca, columns=[f\"pca_{i}\" for i in range(X_pca.shape[1])], index=X.index)\n",
        "        print(f'增加PCA特徵數量: ', len(pca_df.columns))\n",
        "\n",
        "        # 6. 合併\n",
        "        X_all = pd.concat([X_imputed_inver_df, X_interaction_df, pca_df], axis=1)\n",
        "\n",
        "        if self.y_name in df.columns:\n",
        "            # 對齊 y 的 index，只保留 X_all 中有的 index\n",
        "            y_aligned = df[self.y_name].reindex(X_all.index)\n",
        "            # 合併\n",
        "            X_all[self.y_name] = y_aligned\n",
        "            # 刪除 y 為 NaN 的列\n",
        "            X_all = X_all.dropna(subset=[self.y_name])\n",
        "\n",
        "\n",
        "        return X_all\n",
        "\n",
        "    def fit_transform(self, df: pd.DataFrame, y_name=\"SalePrice\", pca_n_components=20):\n",
        "        self.pca_n_components = pca_n_components\n",
        "        self.fit(df, y_name=y_name)\n",
        "        return self.transform(df)\n",
        "\n",
        "    def save(self, save_dir='save_model'):\n",
        "        # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        # file_path = f\"{save_dir}/etl_pipeline_{timestamp}.joblib\"\n",
        "        file_path = f\"{save_dir}/etl_pipeline_RMSLE.joblib\"\n",
        "        dump(self, file_path)\n",
        "        print(f\"ETLPipeline 已儲存到 {file_path}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file_path):\n",
        "        \"\"\"載入已存的 ETLPipeline\"\"\"\n",
        "        obj = load(file_path)\n",
        "        print(f\"ETLPipeline 已從 {file_path} 載入\")\n",
        "        return obj\n",
        "\n",
        "\n",
        "# save_dir = '/content/drive/MyDrive/機器學習練習檔/kaggle房價預測/project/saved_objects'\n",
        "# train_df = pd.read_csv(\"/content/drive/MyDrive/機器學習練習檔/kaggle房價預測/train.csv\")\n",
        "# etl_pipeline = ETLPipeline(save_dir=save_dir)\n",
        "# train_df = etl_pipeline.fit_transform(train_df, pca_n_components=20)\n",
        "# train_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ymzsx9h-dz9"
      },
      "source": [
        "# 4.偏態處理(log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVuR95h0-dXt"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SkeTransX(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    特徵 (X) 轉換器，可選 log/box-cox/yeo-johnson 和標準化方式。\n",
        "    scale_method: 'standard', 'minmax', 或 None\n",
        "    method: 'log', 'box-cox', 'yeo-johnson', 或 None\n",
        "    log_cols_index: 指定列索引進行偏態處理，None 表示所有列\n",
        "    scale_cols_index: 指定列索引進行標準化，None 表示所有列\n",
        "    \"\"\"\n",
        "    def __init__(self, method='yeo-johnson', scale=True, scale_method='standard', log_cols_index=None, scale_cols_index=None):\n",
        "        self.method = method\n",
        "        self.scale = scale\n",
        "        self.scale_method = scale_method\n",
        "        self.log_cols_index = log_cols_index\n",
        "        self.scale_cols_index = scale_cols_index\n",
        "        self.pt = None\n",
        "        self.scaler = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X_copy = np.array(X, dtype=float)\n",
        "\n",
        "        # 偏態處理\n",
        "        if self.method is not None:\n",
        "            if self.method == 'log':\n",
        "                if self.log_cols_index is not None:\n",
        "                    X_copy[:, self.log_cols_index] = np.log1p(X_copy[:, self.log_cols_index])\n",
        "                else:\n",
        "                    X_copy = np.log1p(X_copy)\n",
        "            else:\n",
        "                pt_method = 'box-cox' if self.method == 'box-cox' else 'yeo-johnson'\n",
        "                self.pt = PowerTransformer(method=pt_method, standardize=False)\n",
        "\n",
        "                if self.log_cols_index is not None:\n",
        "                    self.pt.fit(X_copy[:, self.log_cols_index])\n",
        "                    X_copy[:, self.log_cols_index] = self.pt.transform(X_copy[:, self.log_cols_index])\n",
        "                else:\n",
        "                    self.pt.fit(X_copy)\n",
        "                    X_copy = self.pt.transform(X_copy)\n",
        "\n",
        "        # 標準化處理\n",
        "        if self.scale:\n",
        "            if self.scale_method == 'standard':\n",
        "                self.scaler = StandardScaler()\n",
        "            elif self.scale_method == 'minmax':\n",
        "                self.scaler = MinMaxScaler()\n",
        "            else:\n",
        "                self.scaler = None\n",
        "\n",
        "            if self.scaler is not None:\n",
        "                if self.scale_cols_index is not None:\n",
        "                    self.scaler.fit(X_copy[:, self.scale_cols_index])\n",
        "                else:\n",
        "                    self.scaler.fit(X_copy)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = np.array(X, dtype=float)\n",
        "\n",
        "        # 偏態處理\n",
        "        if self.method is not None:\n",
        "            if self.method == 'log':\n",
        "                if self.log_cols_index is not None:\n",
        "                    X_copy[:, self.log_cols_index] = np.log1p(X_copy[:, self.log_cols_index])\n",
        "                else:\n",
        "                    X_copy = np.log1p(X_copy)\n",
        "            else:\n",
        "                if self.log_cols_index is not None:\n",
        "                    X_copy[:, self.log_cols_index] = self.pt.transform(X_copy[:, self.log_cols_index])\n",
        "                else:\n",
        "                    X_copy = self.pt.transform(X_copy)\n",
        "\n",
        "        # 標準化處理\n",
        "        if self.scale and self.scaler is not None:\n",
        "            if self.scale_cols_index is not None:\n",
        "                X_copy[:, self.scale_cols_index] = self.scaler.transform(X_copy[:, self.scale_cols_index])\n",
        "            else:\n",
        "                X_copy = self.scaler.transform(X_copy)\n",
        "\n",
        "        return X_copy\n",
        "\n",
        "\n",
        "class SkeTransY(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    目標變數 (y) 轉換器，可選 log/box-cox/yeo-johnson 和標準化方式。\n",
        "    \"\"\"\n",
        "    def __init__(self, method='yeo-johnson', scale=True, scale_method='standard'):\n",
        "        self.method = method\n",
        "        self.scale = scale\n",
        "        self.scale_method = scale_method\n",
        "        self.pt = None\n",
        "        self.scaler = None\n",
        "\n",
        "    def fit(self, y, *_):\n",
        "        y_arr = np.asarray(y, dtype=float).reshape(-1, 1)\n",
        "\n",
        "        if self.method is not None:\n",
        "            if self.method == 'log':\n",
        "                y_arr = np.log1p(y_arr)\n",
        "            else:\n",
        "                pt_method = 'box-cox' if self.method == 'box-cox' else 'yeo-johnson'\n",
        "                self.pt = PowerTransformer(method=pt_method, standardize=False)\n",
        "                self.pt.fit(y_arr)\n",
        "                y_arr = self.pt.transform(y_arr)\n",
        "\n",
        "        if self.scale:\n",
        "            if self.scale_method == 'standard':\n",
        "                self.scaler = StandardScaler()\n",
        "            elif self.scale_method == 'minmax':\n",
        "                self.scaler = MinMaxScaler()\n",
        "            else:\n",
        "                self.scaler = None\n",
        "\n",
        "            if self.scaler is not None:\n",
        "                self.scaler.fit(y_arr)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, y):\n",
        "        y_arr = np.asarray(y, dtype=float).reshape(-1, 1)\n",
        "\n",
        "        if self.method is not None:\n",
        "            if self.method == 'log':\n",
        "                y_arr = np.log1p(y_arr)\n",
        "            else:\n",
        "                y_arr = self.pt.transform(y_arr)\n",
        "\n",
        "        if self.scale and self.scaler is not None:\n",
        "            y_arr = self.scaler.transform(y_arr)\n",
        "\n",
        "        return y_arr\n",
        "\n",
        "    def inverse_transform(self, y_trans):\n",
        "        y_arr = np.asarray(y_trans, dtype=float).reshape(-1, 1)\n",
        "\n",
        "        if self.scale and self.scaler is not None:\n",
        "            y_arr = self.scaler.inverse_transform(y_arr)\n",
        "\n",
        "        if self.method is not None:\n",
        "            if self.method == 'log':\n",
        "                y_arr = np.expm1(y_arr)\n",
        "            else:\n",
        "                y_arr = self.pt.inverse_transform(y_arr)\n",
        "\n",
        "        return y_arr\n",
        "\n",
        "\n",
        "def get_raw_model(model):\n",
        "    \"\"\"\n",
        "    遞迴剝除包裝器，直到取得最底層的裸模型\n",
        "    支援：\n",
        "        - Pipeline\n",
        "        - TransformedTargetRegressor\n",
        "        - StackingRegressor\n",
        "    \"\"\"\n",
        "    while True:\n",
        "      if isinstance(model, TransformedTargetRegressor):\n",
        "        model = model.regressor\n",
        "      elif isinstance(model, Pipeline):\n",
        "        model = model.steps[-1][1]  # 取最後一層\n",
        "      elif isinstance(model, skStack):\n",
        "        model = model.final_estimator\n",
        "      else:\n",
        "        break  # 已經到底層\n",
        "\n",
        "    return model\n",
        "\n",
        "class MetaFeatureGenerator(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, base_models, cv_folds=5, n_jobs=-1):\n",
        "        self.base_models = base_models\n",
        "        self.cv_folds = cv_folds\n",
        "        self.n_jobs = n_jobs\n",
        "        self.fitted_models_ = {}  # 保存訓練好的 base_models\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        訓練每個 base model 並生成 OOF meta features\n",
        "        \"\"\"\n",
        "        meta_feature = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for name, model in self.base_models.items():\n",
        "            # cross_val_predict 產生 OOF 預測\n",
        "            cv_pred = cross_val_predict(model, X, y, cv=self.cv_folds, n_jobs=self.n_jobs)\n",
        "            meta_feature.append(cv_pred.reshape(-1,1))\n",
        "\n",
        "\n",
        "        self.meta_features_ = np.hstack(meta_feature)\n",
        "        print(f\"Meta features generated in {time.time() - start_time:.2f}s\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        對任意 X 生成 meta features (用已訓練好的 base_models 預測)\n",
        "        \"\"\"\n",
        "        meta_feature = []\n",
        "        for name, model in self.base_models.items():\n",
        "            pred = model.predict(X).reshape(-1,1)\n",
        "            meta_feature.append(pred)\n",
        "\n",
        "        return np.hstack(meta_feature)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.搜尋空間"
      ],
      "metadata": {
        "id": "Zf96PRYAIJO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 快速搜尋\n",
        "fast_search_spaces = {\n",
        "    'rf': [\n",
        "        # ✨ 優化：減少搜尋範圍加速訓練\n",
        "        Integer(50, 200, name='n_estimators'),  # 原: 5-1000 → 50-200\n",
        "        Integer(3, 15, name='max_depth'),       # 原: 1-30 → 3-15\n",
        "        Integer(2, 4, name='min_samples_split'),\n",
        "        Categorical(['sqrt'], name='max_features')  # 原: [None, 'sqrt', 'log2'] → 固定較快選項\n",
        "    ],\n",
        "    'lgb': [\n",
        "        # ✨ 優化：縮小參數範圍\n",
        "        Integer(50, 300, name='n_estimators'),   # 原: 100-2000 → 50-300\n",
        "        Integer(3, 8, name='max_depth'),         # 原: -1-15 → 3-8\n",
        "        Real(0.01, 0.3, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(10, 50, name='num_leaves')       # 原: 10-100 → 10-50\n",
        "    ],\n",
        "    'lr': [\n",
        "        Categorical([True], name='fit_intercept')\n",
        "    ],\n",
        "    'rdg': [\n",
        "        # ✨ 優化：縮小搜尋範圍\n",
        "        Real(0.1, 10, name='alpha', prior='log-uniform')  # 原: 0.01-100 → 0.1-10\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 完整搜尋\n",
        "full_search_spaces = {\n",
        "    'rf_1': [\n",
        "        Integer(10, 500, name='n_estimators'),\n",
        "        Integer(10, 100, name='max_depth'),\n",
        "        Integer(7, 12, name='min_samples_split'),\n",
        "        Categorical([None, 'sqrt', 'log2'], name='max_features')# [None, 'sqrt', 'log2'] 考慮多少特徵去拆分\n",
        "    ],\n",
        "    'rf_2': [\n",
        "        Integer(500, 1000, name='n_estimators'),\n",
        "        Integer(100, 200, name='max_depth'),\n",
        "        Integer(3, 6, name='min_samples_split'),\n",
        "        Categorical([None, 'sqrt', 'log2'], name='max_features')# [None, 'sqrt', 'log2'] 考慮多少特徵去拆分\n",
        "    ],\n",
        "    'dt_1': [\n",
        "        # ✨ 優化：大幅減少深度和範圍\n",
        "        Integer(10, 50, name='max_depth'),        # 原: 20-100 → 5-20\n",
        "        Integer(10, 20, name='min_samples_split'), # 原: 2-20 → 2-10\n",
        "        Integer(5, 20, name='min_samples_leaf'),  # 原: 5-100 → 5-20\n",
        "        Categorical([None, 'sqrt', 'log2'], name='max_features') # 原: [None, 'sqrt', 'log2'] → 固定較快選項\n",
        "    ],\n",
        "    'dt_2': [\n",
        "        # ✨ 優化：大幅減少深度和範圍\n",
        "        Integer(50, 100, name='max_depth'),        # 原: 20-100 → 5-20\n",
        "        Integer(30, 60, name='min_samples_split'), # 原: 2-20 → 2-10\n",
        "        Integer(40, 80, name='min_samples_leaf'),  # 原: 5-100 → 5-20\n",
        "        Categorical([None, 'sqrt', 'log2'], name='max_features') # 原: [None, 'sqrt', 'log2'] → 固定較快選項\n",
        "    ],\n",
        "    # 'mlp': [\n",
        "    #     Categorical(['200_100_50', '1000_500_250_100', '1000_1000_1000_1000'], name='hidden_layer_sizes'),\n",
        "    #     Real(0.0001, 0.1, name='learning_rate_init', prior='log-uniform'),\n",
        "    #     Real(0.0001, 2, name='alpha', prior='log-uniform'),\n",
        "    #     Categorical([16, 32, 64, 128, 256, 512], name='batch_size'),\n",
        "    # ],\n",
        "    'lasso_1': [\n",
        "        # ✨ 優化：縮小搜尋範圍\n",
        "        Real(0.001, 0.01, name='alpha', prior='log-uniform')  # 原: 0.0001-100 → 0.001-10\n",
        "    ],\n",
        "    'lasso_2': [\n",
        "        # ✨ 優化：縮小搜尋範圍\n",
        "        Real(0.01, 0.1, name='alpha', prior='log-uniform')  # 原: 0.0001-100 → 0.001-10\n",
        "    ],\n",
        "    'xgb_1': [\n",
        "        # ✨ 優化：大幅減少估計器數量和深度\n",
        "        Integer(100, 500, name='n_estimators'),   # 原: 2000-4000 → 100-500\n",
        "        Integer(2, 5, name='max_depth'),          # 原: 3-5 → 3-6 但實際更保守\n",
        "        Real(0.01, 0.5, name='learning_rate', prior='log-uniform')  # 調整範圍\n",
        "    ],\n",
        "    'xgb_2': [\n",
        "        # ✨ 優化：大幅減少估計器數量和深度\n",
        "        Integer(600, 1500, name='n_estimators'),   # 原: 2000-4000 → 100-500\n",
        "        Integer(6, 12, name='max_depth'),          # 原: 3-5 → 3-6 但實際更保守\n",
        "        Real(0.01, 0.5, name='learning_rate', prior='log-uniform')  # 調整範圍\n",
        "    ],\n",
        "    'lgb_1': [\n",
        "        # ✨ 優化：減少估計器數量\n",
        "        Integer(100, 500, name='n_estimators'),   # 原: 100-2000 → 100-800\n",
        "        Integer(5, 100, name='max_depth'),        # 原: -1-15 → -1-10\n",
        "        Real(0.001, 0.1, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(3, 100, name='num_leaves')\n",
        "    ],\n",
        "    'lgb_2': [\n",
        "        # ✨ 優化：減少估計器數量\n",
        "        Integer(600, 1200, name='n_estimators'),   # 原: 100-2000 → 100-800\n",
        "        Integer(200, 600, name='max_depth'),        # 原: -1-15 → -1-10\n",
        "        Real(0.001, 0.1, name='learning_rate', prior='log-uniform'),\n",
        "        Integer(3, 100, name='num_leaves')\n",
        "    ],\n",
        "    'lr': [\n",
        "        Categorical([True], name='fit_intercept')\n",
        "    ],\n",
        "    'rdg_1': [\n",
        "        Real(0.0001, 0.001, name='alpha', prior='log-uniform')\n",
        "    ],\n",
        "    'rdg_2': [\n",
        "        Real(0.001, 0.1, name='alpha', prior='log-uniform')\n",
        "    ],\n",
        "    'sgd_1': [\n",
        "        Real(0.0001, 0.01, prior='log-uniform', name='alpha'),\n",
        "        Real(0.001, 0.01, prior='log-uniform', name='l1_ratio'),\n",
        "        Real(0.01, 1, prior='log-uniform', name='epsilon'),\n",
        "        Categorical(['squared_error', 'huber'], name='loss') # ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
        "    ],\n",
        "    'sgd_2': [\n",
        "        Real(0.01, 0.5, prior='log-uniform', name='alpha'),\n",
        "        Real(0.01, 0.9, prior='log-uniform', name='l1_ratio'),\n",
        "        Real(0.01, 1, prior='log-uniform', name='epsilon'),\n",
        "        Categorical(['epsilon_insensitive', 'squared_epsilon_insensitive'], name='loss') # ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
        "    ],\n",
        "    'knn_1': [\n",
        "        # ✨ 優化：減少鄰居數量範圍\n",
        "        Integer(3, 10, name='n_neighbors'),       # 原: 3-20 → 3-15\n",
        "        Categorical(['uniform', 'distance'], name='weights'),  # 原: ['uniform', 'distance'] → 固定較快選項\n",
        "        Categorical([1, 2], name='p')                # 原: [1, 2] → 固定歐氏距離\n",
        "    ],\n",
        "    'knn_2': [\n",
        "        # ✨ 優化：減少鄰居數量範圍\n",
        "        Integer(10, 20, name='n_neighbors'),       # 原: 3-20 → 3-15\n",
        "        Categorical(['uniform', 'distance'], name='weights'),  # 原: ['uniform', 'distance'] → 固定較快選項\n",
        "        Categorical([1, 2], name='p')                # 原: [1, 2] → 固定歐氏距離\n",
        "    ],\n",
        "    'kr_1': [\n",
        "        # ✨ 優化：大幅縮小參數範圍\n",
        "        Real(0.01, 0.1, name='alpha', prior='log-uniform'),     # 原: 0.01-100 → 0.1-10\n",
        "        Real(0.001, 0.01, name='gamma', prior='log-uniform'),     # 原: 0.0001-10 → 0.01-1\n",
        "    ],\n",
        "    'kr_2': [\n",
        "        # ✨ 優化：大幅縮小參數範圍\n",
        "        Real(0.1, 1, name='alpha', prior='log-uniform'),     # 原: 0.01-100 → 0.1-10\n",
        "        Real(0.01, 0.5, name='gamma', prior='log-uniform'),     # 原: 0.0001-10 → 0.01-1\n",
        "    ],\n",
        "    'cat_1': [\n",
        "        Integer(50, 400, name='n_estimators'), # 估計器數量\n",
        "        Integer(3, 7, name='depth'),         # 樹的深度，最大只能到16\n",
        "        Real(0.001, 0.1, name='learning_rate', prior='log-uniform'), # 學習率\n",
        "        Real(0.01, 1, name='l2_leaf_reg', prior='log-uniform') # L2 正則化\n",
        "    ],\n",
        "    'cat_2': [\n",
        "        Integer(700, 1000, name='n_estimators'), # 估計器數量\n",
        "        Integer(3, 7, name='depth'),         # 樹的深度，最大只能到16\n",
        "        Real(0.001, 0.1, name='learning_rate', prior='log-uniform'), # 學習率\n",
        "        Real(1, 10, name='l2_leaf_reg', prior='log-uniform') # L2 正則化\n",
        "    ],\n",
        "    'svr_1': [\n",
        "        Real(0.01, 1, name='C', prior='log-uniform'),  # 正則化參數\n",
        "        Real(0.001, 0.1, name='gamma', prior='log-uniform'), # 核函數係數\n",
        "    ],\n",
        "    'svr_2': [\n",
        "        Real(1, 100, name='C', prior='log-uniform'),  # 正則化參數\n",
        "        Real(0.1, 1, name='gamma', prior='log-uniform'), # 核函數係數\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "KoQt6kM_IJEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eNlvLHzTnxd"
      },
      "source": [
        "# 6.Stacking Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1GZ6Qr2TDjq"
      },
      "outputs": [],
      "source": [
        "class StackingModel:\n",
        "    def __init__(self, x, y, random_state=42, search_ncall=10, cv_folds=5, test_size=0.2, fast_models_only=False, interx=None):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        etl = ETLPipeline()\n",
        "        self.con_inter = etl.con_var + interx\n",
        "        self.skew_col = [s for s in self.con_inter if s in self.x.columns and self.x[s].skew() > 0.5]\n",
        "        self.log_cols_index = [self.x.columns.get_loc(c) for c in self.skew_col]\n",
        "        self.scale_cols_index = [self.x.columns.get_loc(c) for c in self.x.columns if 'pca' not in c]\n",
        "\n",
        "        self.random_state = random_state\n",
        "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x, self.y, test_size=test_size, random_state=self.random_state)\n",
        "\n",
        "        self.search_ncall = search_ncall\n",
        "        # ✨ 新增屬性：可配置的加速選項\n",
        "        self.cv_folds = cv_folds  # 減少 CV folds\n",
        "        self.fast_models_only = fast_models_only\n",
        "\n",
        "        # ✨ 針對低核心數CPU的智能並行配置\n",
        "        self.n_cpu = os.cpu_count() or 4\n",
        "\n",
        "        # 🎯 專門針對2核CPU優化的配置\n",
        "        if self.n_cpu <= 2:\n",
        "            # 2核CPU：採用序列模型搜尋，但單模型內部並行\n",
        "            self.outer_n_jobs = 1    # 一次只訓練一個模型\n",
        "            self.inner_n_jobs = 2    # 單模型使用全部核心\n",
        "            print(f\"⚡ 低核心CPU模式：序列搜尋模型，單模型使用{self.inner_n_jobs}核心\")\n",
        "        elif self.n_cpu <= 4:\n",
        "            # 4核CPU：適度並行\n",
        "            self.outer_n_jobs = 2    # 同時訓練2個模型\n",
        "            self.inner_n_jobs = 2    # 每個模型用2核心\n",
        "            print(f\"⚡ 中等CPU模式：{self.outer_n_jobs}個模型並行，每個用{self.inner_n_jobs}核心\")\n",
        "        else:\n",
        "            # 高核心CPU：正常配置\n",
        "            self.outer_n_jobs = min(4, max(2, self.n_cpu // 2))\n",
        "            self.inner_n_jobs = max(1, self.n_cpu // self.outer_n_jobs)\n",
        "            print(f\"⚡ 高性能CPU模式：{self.outer_n_jobs}個模型並行，每個用{self.inner_n_jobs}核心\")\n",
        "\n",
        "        # ✨ 新增：根據模式選擇搜尋空間\n",
        "        self.search_spaces = fast_search_spaces if fast_models_only else full_search_spaces\n",
        "        self.standard_method = 'minmax' # standard / minmax 用於偏態處理時\n",
        "        self.standard_stack = StandardScaler() # MinMaxScaler() StandardScaler()\n",
        "        self.trans_skew_type = 'yeo-johnson' # 'log' / 'box-cox' / 'yeo-johnson'\n",
        "\n",
        "    def evaluate_model(self, name, params, stack=False, input_x=None): #name, space, stack=False, estimators=None, input_x=None\n",
        "        try:\n",
        "            if name.startswith('rf'):\n",
        "                model = rf(random_state=self.random_state, n_jobs=self.inner_n_jobs, **params) # ** 為解開dict讓param的dict的key去對應模型參數名稱value作為參數設定\n",
        "            elif name.startswith('dt'):\n",
        "                model = dt(random_state=self.random_state, **params)\n",
        "            elif name.startswith('lasso'):\n",
        "                model = Lasso(max_iter=2000, **params)\n",
        "            elif name.startswith('xgb'):\n",
        "                model = xgb.XGBRegressor(random_state=self.random_state, objective='reg:squarederror', n_jobs=self.inner_n_jobs, tree_method='hist', **params)\n",
        "            elif name.startswith('lgb'):\n",
        "                model = lgb.LGBMRegressor(random_state=self.random_state, verbose=-1, n_jobs=self.inner_n_jobs, **params)\n",
        "            elif name.startswith('lr'):\n",
        "                model = lr(n_jobs=self.inner_n_jobs, **params)\n",
        "            elif name.startswith('rdg'):\n",
        "                model = rdg(**params)\n",
        "            elif name.startswith('sgd'):\n",
        "                model = sgd(random_state=self.random_state, learning_rate='adaptive', **params)\n",
        "            elif name.startswith('knn'):\n",
        "                model = knn(n_jobs=self.inner_n_jobs, **params)\n",
        "            elif name.startswith('kr'):\n",
        "                model = kr(kernel='rbf', **params)\n",
        "            elif name.startswith('svr'):\n",
        "                model = svr(**params)\n",
        "            elif name.startswith('cat'):\n",
        "                model = cb.CatBoostRegressor(random_state=self.random_state, verbose=0, thread_count=self.inner_n_jobs, early_stopping_rounds=100, **params)\n",
        "            else:\n",
        "                return 1e6\n",
        "\n",
        "            cv_jobs = 1 if self.n_cpu <= 2 and self.cv_folds >= 3 else min(self.cv_folds, self.inner_n_jobs)\n",
        "\n",
        "            if stack:\n",
        "                pipeline = Pipeline([\n",
        "                    ('Trans', SkeTransX(method=self.trans_skew_type, scale=True, scale_method=self.standard_method)),\n",
        "                    ('model', model)\n",
        "                ])\n",
        "                y_transformer = SkeTransY(method=self.trans_skew_type, scale=True, scale_method=self.standard_method)\n",
        "                model = TransformedTargetRegressor(regressor=pipeline, transformer=y_transformer)\n",
        "                score = cross_val_score(model, input_x, self.y_train, cv=self.cv_folds,\n",
        "                                        scoring='neg_root_mean_squared_error', n_jobs=cv_jobs).mean()\n",
        "            else:\n",
        "                best_pipeline = Pipeline([\n",
        "                    ('Trans', SkeTransX(method=self.trans_skew_type, scale=True, scale_method=self.standard_method,\n",
        "                                        log_cols_index=self.log_cols_index, scale_cols_index=self.scale_cols_index)),\n",
        "                    ('model', model)])\n",
        "                y_transformer = SkeTransY(method=self.trans_skew_type, scale=True, scale_method=self.standard_method)\n",
        "                model = TransformedTargetRegressor(regressor=best_pipeline, transformer=y_transformer)\n",
        "                score = cross_val_score(model, self.x_train, self.y_train, cv=self.cv_folds,\n",
        "                                        scoring='neg_root_mean_squared_log_error', n_jobs=cv_jobs).mean()\n",
        "                                            # neg_root_mean_squared_log_error neg_root_mean_squared_error\n",
        "            return score\n",
        "        except Exception as e:\n",
        "            print(f\"objective Error in {name}: {e}\")\n",
        "            return 1e6\n",
        "\n",
        "    # objective = self.make_objective_optuna(name, space, stack=stack, estimators=estimators, input_x=input_x)\n",
        "    def make_objective_optuna(self, name, space, stack=False, estimators=None, input_x=None, meta_g=None):\n",
        "        def objective(trial):\n",
        "            params = {}\n",
        "            for p in space:\n",
        "                if isinstance(p, Integer):\n",
        "                    params[p.name] = trial.suggest_int(p.name, p.low, p.high)\n",
        "                elif isinstance(p, Real):\n",
        "                    params[p.name] = trial.suggest_float(p.name, p.low, p.high, log=(p.prior == \"log-uniform\"))\n",
        "                elif isinstance(p, Categorical):\n",
        "                    params[p.name] = trial.suggest_categorical(p.name, p.categories)\n",
        "            return self.evaluate_model(name, params, stack, input_x)\n",
        "        return objective\n",
        "\n",
        "    def best_model_(self, params, name, stack=False, estimators=None, input_x=None, meta_g=None):\n",
        "        # 用最佳參數建立模型\n",
        "        if name.startswith('rf'):\n",
        "            model = rf(random_state=self.random_state,n_jobs=self.inner_n_jobs, **params)\n",
        "        elif name.startswith('dt'):\n",
        "            model = dt(random_state=self.random_state, **params)\n",
        "        # elif name == 'mlp':\n",
        "        #     model = mlp(random_state=42, max_iter=500, hidden_layer_sizes=(1000,1000,1000,1000,1000,1000), alpha=0.0001, batch_size=128,\n",
        "        #                   learning_rate_init=0.0001, learning_rate='adaptive', early_stopping=True)\n",
        "        elif name.startswith('lasso'):\n",
        "            model = Lasso(max_iter=2000, **params)\n",
        "        elif name.startswith('xgb'):\n",
        "            model = xgb.XGBRegressor(random_state=self.random_state, objective='reg:squarederror', n_jobs=self.inner_n_jobs, tree_method='hist', **params)\n",
        "        elif name.startswith('lgb'):\n",
        "            model = lgb.LGBMRegressor(random_state=self.random_state, verbose=-1,n_jobs=self.inner_n_jobs, **params)\n",
        "        elif name.startswith('lr'):\n",
        "            model = lr(n_jobs=self.inner_n_jobs, **params)\n",
        "        elif name.startswith('rdg'):\n",
        "            model = rdg(**params)\n",
        "        elif name.startswith('sgd'):\n",
        "            model = sgd(random_state=self.random_state, learning_rate='adaptive', **params)\n",
        "        elif name.startswith('knn'):\n",
        "            model = knn(n_jobs=self.inner_n_jobs, **params)\n",
        "        elif name.startswith('kr'):\n",
        "            model = kr(kernel='rbf', **params)\n",
        "        elif name.startswith('svr'):\n",
        "            model = svr(**params)\n",
        "        elif name.startswith('cat'):\n",
        "            model = cb.CatBoostRegressor(random_state=self.random_state, verbose=0, thread_count=self.inner_n_jobs, **params)\n",
        "\n",
        "        if stack:                    # StandardScaler() MinMaxScaler()\n",
        "            pipeline = Pipeline([('Trans', SkeTransX(method=self.trans_skew_type, scale=True,scale_method=self.standard_method,)),('model', model)])\n",
        "            y_transformer = SkeTransY(method=self.trans_skew_type, scale=True, scale_method=self.standard_method)\n",
        "            final_model = TransformedTargetRegressor(regressor=pipeline, transformer=y_transformer)\n",
        "            final_model.fit(input_x, self.y_train)\n",
        "\n",
        "\n",
        "        else:\n",
        "            # 基學習器使用完整轉換\n",
        "            # # 目標變數 (y) 轉換器，可選 log/box-cox/yeo-johnson 和標準化方式 standard / minmax\n",
        "            pipeline = Pipeline([('Trans', SkeTransX(method=self.trans_skew_type, scale=True,scale_method=self.standard_method,\n",
        "                                                     log_cols_index=self.log_cols_index, scale_cols_index=self.scale_cols_index)),\n",
        "                        ('model', model)])\n",
        "            y_transformer = SkeTransY(method=self.trans_skew_type, scale=True, scale_method=self.standard_method)\n",
        "            final_model = TransformedTargetRegressor(regressor=pipeline, transformer=y_transformer)\n",
        "            final_model.fit(self.x_train, self.y_train)\n",
        "\n",
        "        return final_model\n",
        "\n",
        "    def search_param(self, stack=False, meta_models_to_try=None, estimators=None, input_x=None, meta_g=None):\n",
        "        # 選擇要搜尋的模型\n",
        "        print('search model params ...')\n",
        "        if not stack:\n",
        "            search_spaces = self.search_spaces\n",
        "        else:\n",
        "            search_spaces = {name: self.search_spaces[name] for name in meta_models_to_try if name in self.search_spaces}\n",
        "\n",
        "        result = {}\n",
        "\n",
        "        def tune_model(name, space):\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                print(f\"Start tuning {name} with Optuna study name: {name}_optimization\")\n",
        "                study = optuna.create_study(direction='maximize', study_name=f'{name}_optimization', storage=None,\n",
        "                                            sampler=optuna.samplers.TPESampler(seed=self.random_state))\n",
        "                # objective = self.make_objective_gp(name, space, stack=stack, estimators=estimators, input_x=input_x)\n",
        "                objective = self.make_objective_optuna(name, space, stack=stack, estimators=estimators, input_x=input_x)\n",
        "\n",
        "                if name == 'mlp':\n",
        "                  actual_calls = 3\n",
        "                else:\n",
        "                  actual_calls = max(3, self.search_ncall)  # 限制搜尋次數\n",
        "\n",
        "                if stack and len(meta_models_to_try) == 1:\n",
        "                  actual_calls = actual_calls * 5\n",
        "\n",
        "                study.optimize(objective, n_trials=actual_calls, timeout=3000, show_progress_bar=True)\n",
        "\n",
        "                # res = gp_minimize(objective, dimensions=space, n_calls=actual_calls, n_random_starts=3, acq_func='EI',\n",
        "                #                   random_state=self.random_state, n_points=5000, n_initial_points=5) # 這種貝葉斯適合stack\n",
        "                second_time = time.time()\n",
        "\n",
        "                best_model = self.best_model_(study.best_params, name, stack=stack, estimators=estimators, input_x=input_x, meta_g=meta_g)\n",
        "\n",
        "                if stack:\n",
        "                  best_model = Pipeline([('meta_g', meta_g), ('model', best_model)])\n",
        "                  y_pred = best_model.predict(self.x_test)\n",
        "                else:\n",
        "                  y_pred = best_model.predict(self.x_test)\n",
        "\n",
        "                rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))\n",
        "                mae = mean_absolute_error(self.y_test, y_pred)\n",
        "                rmsle = np.sqrt(mean_squared_log_error(self.y_test, y_pred))\n",
        "                r2 = r2_score(self.y_test, y_pred)\n",
        "\n",
        "                elapsed1 = second_time - start_time\n",
        "                elapsed2 = time.time() - start_time\n",
        "\n",
        "                r2_threshold = 0.2 if self.fast_models_only else 0.85  # 原: 固定 0.4\n",
        "                if r2 < r2_threshold:\n",
        "                    print(f\"{name} removed due to low R2 ({r2:.4f})\")\n",
        "                    return None\n",
        "\n",
        "                print(f\"\\033[1m{'stack_' if stack else ''}{name} | RMSE: {rmse:.4f} | RMSLE: {rmsle:.4f} | R2: {r2:.4f} | SearchTime: {elapsed1:.1f}s | FitTime: {elapsed2:.1f}s\\033[0m\")\n",
        "\n",
        "                metrics = {\"RMSE\": rmse, \"MAE\": mae, \"RMSLE\": rmsle, \"R2\": r2}\n",
        "                # p = study.best_params.copy()\n",
        "                param_names = [dim.name for dim in space]   # 取得每個維度名稱\n",
        "                best_params_dict = study.best_params # Optuna的 best_params 本身就是字典\n",
        "                print(best_params_dict)\n",
        "                return name, best_model, metrics, best_params_dict\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error tuning {name}: {e}\")\n",
        "                # traceback.print_exc()\n",
        "                return None\n",
        "\n",
        "        # ✨ 2核CPU的外層並行策略\n",
        "        if self.n_cpu <= 2:\n",
        "            print(f\"🐌 2核CPU模式：序列搜尋{len(search_spaces)}個{'堆疊' if stack else '基礎'}模型（避免競爭）\")\n",
        "            print(f\"   每個模型內部使用{self.inner_n_jobs}個線程\")\n",
        "        else:\n",
        "            print(f\"⚡ 並行搜尋{len(search_spaces)}個{'堆疊' if stack else '基礎'}模型，外層{self.outer_n_jobs}並行 × 內層{self.inner_n_jobs}線程\")\n",
        "\n",
        "        results = Parallel(n_jobs=self.outer_n_jobs)(delayed(tune_model)(name, space) for name, space in search_spaces.items())\n",
        "\n",
        "        # 收集結果\n",
        "        base_models = {}\n",
        "        params = {}\n",
        "        for r in results:\n",
        "            if r is not None:\n",
        "                name, best_model, metrics, best_params_dict = r\n",
        "                base_models[name] = best_model\n",
        "                result[name] = metrics\n",
        "                params[name] = best_params_dict\n",
        "\n",
        "\n",
        "        if stack:\n",
        "            if result:\n",
        "                best_name = min(result, key=lambda x: result[x][\"RMSLE\"])\n",
        "                best_meta_model = base_models[best_name]\n",
        "                best_meta_metrics = result[best_name]\n",
        "                best_meta_params = params[best_name]\n",
        "                best_meta_model1 = get_raw_model(best_meta_model)\n",
        "                print(f\"Best meta model: {type(best_meta_model1).__name__} - Evaluation: {best_meta_metrics} - params:{best_meta_params}\")\n",
        "                return best_meta_model, best_meta_metrics, best_meta_params\n",
        "            else:\n",
        "                print(\"No suitable meta model found, using Ridge regression\")\n",
        "                meta_pipeline = Pipeline([('Trans', SkeTransX(method=self.trans_skew_type, scale=True, scale_method=self.standard_method, log_cols_index=self.log_cols_index,\n",
        "                                    scale_cols_index=self.scale_cols_index)), ('model', skStack(estimators=estimators, final_estimator=rdg(), n_jobs=self.inner_n_jobs))])\n",
        "                y_transformer = SkeTransY(method=self.trans_skew_type, scale=True, scale_method=self.standard_method)\n",
        "                best_meta_model = TransformedTargetRegressor(regressor=meta_pipeline, transformer=y_transformer)\n",
        "                best_meta_model.fit(self.x_train, self.y_train)\n",
        "\n",
        "                y_pred = best_meta_model.predict(self.x_test)\n",
        "\n",
        "                rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))\n",
        "                mae = mean_absolute_error(self.y_test, y_pred)\n",
        "                rmsle = np.sqrt(mean_squared_log_error(self.y_test, y_pred))\n",
        "                r2 = r2_score(self.y_test, y_pred)\n",
        "                best_meta_metrics = {\"RMSE\": rmse, \"MAE\": mae, \"RMSLE\": rmsle,\"R2\": r2}\n",
        "                best_meta_params = None\n",
        "                return best_meta_model, best_meta_metrics, best_meta_params\n",
        "        else:\n",
        "            return base_models, result, params\n",
        "\n",
        "    def fit(self, meta_model=3):\n",
        "        print(f\"Starting {'fast' if self.fast_models_only else 'full'} model search...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        base_models, result, params = self.search_param()\n",
        "\n",
        "        if not base_models:\n",
        "            print(\"No suitable base models found!\")\n",
        "            return None\n",
        "\n",
        "        # 儲存基模型和結果\n",
        "        self.base_models = base_models\n",
        "        self.results = result\n",
        "        self.params = params\n",
        "\n",
        "        print(f\"Base model search completed in {time.time() - start_time:.1f}s\")\n",
        "        print(f\"Found {len(base_models)} suitable base models: {list(base_models.keys())}\")\n",
        "\n",
        "        # Stacking\n",
        "        estimators = [(name, model) for name, model in base_models.items()]\n",
        "\n",
        "        print('Starting stacking fit...')\n",
        "        stack_start = time.time()\n",
        "\n",
        "        best_name = min(result, key=lambda x: result[x]['RMSLE'])\n",
        "\n",
        "        # 先計算x_meta矩陣，也就是給 meta model 的輸入\n",
        "        self.meta_g = MetaFeatureGenerator(base_models)\n",
        "        x_meta = self.meta_g.fit(self.x_train, self.y_train).transform(self.x_train)\n",
        "\n",
        "        if meta_model == 1 :\n",
        "            best_base_model = base_models[best_name]\n",
        "            self.best_result = result[best_name]\n",
        "            self.best_params = params[best_name]\n",
        "\n",
        "            meta_model = get_raw_model(best_base_model)\n",
        "            print(f\"use meta model: {type(meta_model).__name__} \\n - Evaluation: {best_result}\\n - best_params: {best_params}\")\n",
        "            meta_pipeline = Pipeline([('Trans', SkeTransX(method=self.trans_skew_type, scale=True,scale_method=self.standard_method, log_cols_index=self.log_cols_index, scale_cols_index=self.scale_cols_index)),\n",
        "                                                            ('model', skStack(estimators=estimators, final_estimator=meta_model, n_jobs=self.inner_n_jobs, cv=self.cv_folds))])\n",
        "            y_transformer = SkeTransY(method=self.trans_skew_type, scale=True, scale_method=self.standard_method)\n",
        "            self.final_model = TransformedTargetRegressor(regressor=meta_pipeline, transformer=y_transformer)\n",
        "            self.final_model.fit(self.x_train, self.y_train)\n",
        "\n",
        "\n",
        "        elif meta_model == 2:\n",
        "            meta_models_to_try = [best_name]\n",
        "            final_model, best_result, best_params = self.search_param(stack=True, meta_models_to_try=meta_models_to_try, estimators=estimators, input_x=x_meta, meta_g=self.meta_g)\n",
        "            meta_model = get_raw_model(final_model)\n",
        "            # final_model_f = Pipeline([('meta_feature', meta_g), ('final_model', final_model)])\n",
        "            final_model.fit(self.x_train, self.y_train)\n",
        "            self.final_model = final_model\n",
        "            self.best_result = best_result\n",
        "            self.best_params = best_params\n",
        "            print(f\"use meta model: {type(meta_model).__name__} \\n - Evaluation: {best_result}\\n - best_params: {best_params}\")\n",
        "\n",
        "        elif meta_model == 3:\n",
        "            meta_models_to_try = list(base_models.keys())\n",
        "            final_model, best_result, best_params = self.search_param(stack=True, meta_models_to_try=meta_models_to_try, estimators=estimators, input_x=x_meta, meta_g=self.meta_g)\n",
        "            meta_model = get_raw_model(final_model)\n",
        "            # final_model_f = Pipeline([('meta_feature', meta_g), ('final_model', final_model)])\n",
        "            final_model.fit(self.x_train, self.y_train)\n",
        "            self.final_model = final_model\n",
        "            self.best_result = best_result\n",
        "            self.best_params = best_params\n",
        "            print(f\"use meta model: {type(meta_model).__name__} \\n - Evaluation: {best_result}\\n - best_params: {best_params}\")\n",
        "\n",
        "        else:\n",
        "            print(\"請正確輸入meta_model參數\")\n",
        "\n",
        "        # 預測\n",
        "        y_pred_stack = self.final_model.predict(self.x_test)\n",
        "        # 計算誤差等\n",
        "        rmse_stack = np.sqrt(mean_squared_error(self.y_test, y_pred_stack))\n",
        "        mae_stack = mean_absolute_error(self.y_test, y_pred_stack)\n",
        "        rmsle_stack = np.sqrt(mean_squared_log_error(self.y_test, y_pred_stack))\n",
        "        r2_stack = r2_score(self.y_test, y_pred_stack)\n",
        "        self.results['stacking'] = {\"RMSE\": rmse_stack, \"MAE\": mae_stack, \"RMSLE\": rmsle_stack, \"R2\": r2_stack}\n",
        "        self.params['stacking'] = best_params\n",
        "\n",
        "        print(f\"Final Stacking - Evaluation: {self.results['stacking']}\")\n",
        "        print(f\"Stacking completed in {time.time() - stack_start:.1f}s\")\n",
        "        print(f\"Total training time: {time.time() - start_time:.1f}s\")\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"使用已訓練的 stacking model 預測，並可選擇輸出到檔案\"\"\"\n",
        "        if not hasattr(self, 'final_model'):\n",
        "            raise AttributeError(\"Model is not fitted. Call fit() first.\")\n",
        "\n",
        "        preds = self.final_model.predict(X)\n",
        "        return preds\n",
        "    def save(self, output_path):\n",
        "        # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        # model_path = os.path.join(output_path, f\"model_{timestamp}.joblib\")\n",
        "        # txt_path = os.path.join(output_path, f\"model_{timestamp}.txt\")\n",
        "        model_path = os.path.join(output_path, \"model_RMSLE.joblib\")\n",
        "        txt_path = os.path.join(output_path, \"result_RMSLE.txt\")\n",
        "        dump(self.final_model, model_path)\n",
        "        # 輸出結果\n",
        "        if output_path:\n",
        "          with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            for name, metrics in self.results.items():\n",
        "              if name != 'stacking':\n",
        "                f.write(f\"{name} - Evaluation: {metrics}\\n\")\n",
        "                f.write(f\"{name} - best_params: {self.params[name]}\\n\")\n",
        "\n",
        "            f.write(f\"\\nFinal Stacking - Evaluation: {self.results['stacking']}\\n\")\n",
        "            print(f\" MODEL 已儲存到 {model_path}\")\n",
        "\n",
        "    def get_base_results(self):\n",
        "        \"\"\"取得所有 base model 評估結果\"\"\"\n",
        "        return self.base_results\n",
        "\n",
        "    def get_base_params(self):\n",
        "        \"\"\"取得所有 base model 最佳參數\"\"\"\n",
        "        return self.base_params\n",
        "\n",
        "    def get_meta_result(self):\n",
        "        \"\"\"取得 meta model 評估結果與最佳參數\"\"\"\n",
        "        return self.best_result\n",
        "\n",
        "    def get_meta_params(self):\n",
        "        \"\"\"取得 meta model 評估結果與最佳參數\"\"\"\n",
        "        return self.best_params\n",
        "\n",
        "    def get_final_model(self):\n",
        "        \"\"\"取得最終 stacking 模型\"\"\"\n",
        "        return self.final_model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    save_dir = '/content/drive/MyDrive/機器學習練習檔/kaggle房價預測/project/saved_model'\n",
        "\n",
        "    # ====== data training =============================\n",
        "    train_df = pd.read_csv(\"/content/drive/MyDrive/機器學習練習檔/kaggle房價預測/train.csv\")\n",
        "    etl_pipeline = ETLPipeline()\n",
        "    train_df = etl_pipeline.fit_transform(train_df, y_name=\"SalePrice\", pca_n_components=20)\n",
        "    etl_pipeline.save(save_dir)\n",
        "\n",
        "    X_train = train_df.drop(columns=['SalePrice'])\n",
        "    y_train = train_df['SalePrice']\n",
        "    X_train = X_train.astype(np.float64)\n",
        "    y_train = y_train.astype(np.float64)\n",
        "    # df = X_train.copy()\n",
        "    # df[\"target\"] = y_train\n",
        "    # df_sample = df.sample(n=100, random_state=41)\n",
        "    # x_train_all = df_sample.drop(columns=[\"target\"])\n",
        "    # y_train = df_sample[\"target\"]\n",
        "\n",
        "    model = StackingModel(X_train, y_train, search_ncall=10, cv_folds=5, interx=etl_pipeline.X_interaction_list) # scale_col\n",
        "    model.fit(meta_model=3)\n",
        "    model.save(output_path=save_dir)\n",
        "\n",
        "    # ====== data predict =============================\n",
        "    test_df = pd.read_csv(\"/content/drive/MyDrive/機器學習練習檔/kaggle房價預測/test.csv\")\n",
        "    etl_pipeline_pred = ETLPipeline.load('/content/drive/MyDrive/機器學習練習檔/kaggle房價預測/project/saved_model/etl_pipeline_RMSLE.joblib')\n",
        "    X_test = etl_pipeline_pred.transform(test_df)\n",
        "    X_test = X_test.astype(np.float64)\n",
        "\n",
        "    model_path = os.path.join(save_dir, 'model_RMSLE.joblib')\n",
        "    model = load(model_path)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    result = pd.DataFrame({\"Id\": test_df[\"Id\"], \"SalePrice\": y_pred})\n",
        "    result.to_csv(\"/content/drive/MyDrive/機器學習練習檔/kaggle房價預測/project/result/prediction.csv\", index=False)\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxFekkxagKL2"
      },
      "source": [
        "# 預測"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_AHcGCSJ8IF"
      },
      "outputs": [],
      "source": [
        "# 讀取模型\n",
        "# loaded_model = load(\"trained_model.joblib\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1akXK5SNGVnYpmkPguugAc-QWYYwSzS6o",
      "authorship_tag": "ABX9TyMklKcwbn4KPMUZhyuoRr0I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}